{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10318150,"sourceType":"datasetVersion","datasetId":6388053}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns; sns.set()\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, LSTM, Bidirectional\nfrom keras.utils import plot_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nnumber_of_samples = 100000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\ndata_attack = pd.read_csv('/kaggle/input/dataset1/dataset_attack.csv', nrows = number_of_samples)\n\ndata_normal = pd.read_csv('/kaggle/input/dataset1/dataset_normal.csv', nrows = number_of_samples)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndata_normal.columns=[ 'frame.len', 'frame.protocols', 'ip.hdr_len',\n       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n       'tcp.time_delta','class']\ndata_attack.columns=[ 'frame.len', 'frame.protocols', 'ip.hdr_len',\n       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n       'tcp.time_delta','class']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndata_normal=data_normal.drop(['ip.src', 'ip.dst','frame.protocols'],axis=1)\ndata_attack=data_attack.drop(['ip.src', 'ip.dst','frame.protocols'],axis=1)\n\nfeatures=[ 'frame.len', 'ip.hdr_len',\n       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n       'ip.ttl', 'ip.proto', 'tcp.srcport', 'tcp.dstport',\n       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n       'tcp.time_delta']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_normal= data_normal[features].values\nX_attack= data_attack[features].values\nY_normal= data_normal['class']\nY_attack= data_attack['class']\nX=np.concatenate((X_normal,X_attack))\nY=np.concatenate((Y_normal,Y_attack))\n\nscalar = StandardScaler(copy=True, with_mean=True, with_std=True)\nscalar.fit(X)\nX = scalar.transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor i in range(0,len(Y)):\n  if Y[i] ==\"attack\":\n    Y[i]=0\n  else:\n    Y[i]=1\n\nprint(X.shape)\n\nfeatures = len(X[0])\nsamples = X.shape[0]\ntrain_len = 25\ninput_len = samples - train_len\nI = np.zeros((samples - train_len, train_len, features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor i in range(input_len):\n    temp = np.zeros((train_len, features))\n    for j in range(i, i + train_len):\n        temp[j-i] = X[j]\n    I[i] = temp\n\nprint(I.shape);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(I, Y[25:200000], test_size = 0.2)\nX_train = X_train.astype(np.float32)\nY_train = Y_train.astype(np.float32)\nX_test = X_test.astype(np.float32)\nY_test = Y_test.astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, GRU, MultiHeadAttention, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\ndef create_complex_model():\n    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n\n\n    cnn_out = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(input_layer)\n    residual = cnn_out\n    cnn_out = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(cnn_out)\n    cnn_out = Add()([cnn_out, residual])\n    cnn_out = MaxPooling1D(pool_size=2)(cnn_out)\n    cnn_out = Dropout(0.3)(cnn_out)\n\n\n    gru_out = Bidirectional(GRU(256, return_sequences=True, kernel_regularizer='l2'))(cnn_out)\n    gru_out = Dropout(0.3)(gru_out)\n    \n\n\n\n    attention_out = MultiHeadAttention(num_heads=16, key_dim=32)(gru_out, gru_out)\n\n\n    global_avg_pooling = GlobalAveragePooling1D()(attention_out)\n    global_max_pooling = GlobalMaxPooling1D()(attention_out)\n    concatenated = concatenate([global_avg_pooling, global_max_pooling])\n\n\n    dense_out = Dense(256, activation='swish', kernel_regularizer='l2')(concatenated)\n    dense_out = Dropout(0.3)(dense_out)\n    dense_out = Dense(256, activation='swish', kernel_regularizer='l2')(dense_out)\n    dense_out = Dropout(0.3)(dense_out)\n\n\n    output_layer = Dense(1, activation='sigmoid')(dense_out)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n\n    model.compile(loss='binary_crossentropy', optimizer=AdamW(learning_rate=0.001, weight_decay=1e-5), metrics=['accuracy'])\n\n    return model\n\nmodel = create_complex_model()\nlr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n\n\nhistory = model.fit(X_train, Y_train, epochs=500, validation_split=0.2, verbose=1, callbacks=[lr_schedule])\nmodel.summary()\n\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='lower right')\nplt.savefig('Model Accuracy.png')\nplt.show()\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model  Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig('Model Loss.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict = model.predict(X_test, verbose=1)\n\ntp = 0\ntn = 0\nfp = 0\nfn = 0\npredictn = predict.flatten().round()\npredictn = predictn.tolist()\nY_testn = Y_test.tolist()\nfor i in range(len(Y_testn)):\n  if predictn[i]==1 and Y_testn[i]==1:\n    tp+=1\n  elif predictn[i]==0 and Y_testn[i]==0:\n    tn+=1\n  elif predictn[i]==0 and Y_testn[i]==1:\n    fp+=1\n  elif predictn[i]==1 and Y_testn[i]==0:\n    fn+=1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nto_heat_map =[[tn,fp],[fn,tp]]\nto_heat_map = pd.DataFrame(to_heat_map, index = [\"Attack\",\"Normal\"],columns = [\"Attack\",\"Normal\"])\nax = sns.heatmap(to_heat_map,annot=True, fmt=\"d\")\n\nX_test = np.asarray(X_test, dtype=np.float32)\nY_test = np.asarray(Y_test, dtype=np.float32)\nscores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\npredictions_proba = model.predict(X_test)\npredictions = (predictions_proba > 0.5).astype(int)\n\naccuracy = accuracy_score(Y_test, predictions)\n\nerror_rate = 1 - accuracy\n\nprecision = precision_score(Y_test, predictions)\n\nrecall = recall_score(Y_test, predictions)\n\nf1 = f1_score(Y_test, predictions)\n\nauc = roc_auc_score(Y_test, predictions_proba)\n\nprint(f\"Error Rate: {error_rate:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"AUC: {auc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nstart_time = time.time()\ny_pred = model.predict(X_test)\nend_time = time.time()\nprint(end_time - start_time)\nprint(f\"Inference time per batch: {(end_time - start_time)/len(X_test):.6f} seconds\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}